{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92fb9b3-bbf8-418a-84c6-0987d046fea3",
   "metadata": {},
   "source": [
    "### To do list:\n",
    "#### 1. ~~Скачать датасет *VGGSound*. Ссылка: https://huggingface.co/datasets/Loie/VGGSound/tree/main~~\n",
    "#### 2. ~~Все видео-файлы заменить на случайный кадр из них~~\n",
    "#### 2.1 ~~Перевести wav файлы к частоте 48кГц~~\n",
    "#### 3. ~~Реализовать DataLoader wav->ipeg~~\n",
    "#### 4. ~~Встроить аудио-энкодер. Ссылка: https://github.com/archinetai/archisound~~\n",
    "#### 5. ~~Реализовать сценарий обучения и обучить модель~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b2550-b41e-4952-b575-83bc645fc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.config import ModelConfig\n",
    "from models.unet import UNetWithCrossAttention\n",
    "from models.diffusion import Diffusion\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.SoundDataset import SoundDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877fb987-318f-43a5-a189-b77e18dead79",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/images\"\n",
    "sound_path = \"data/sounds\"\n",
    "\n",
    "data = SoundDataset(image_path, sound_path)\n",
    "train_data, val_data = torch.utils.data.random_split(data, [197889-10000, 10000])\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size=96,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True, \n",
    "                          drop_last=True)\n",
    "                         \n",
    "val_loader = DataLoader(val_data, \n",
    "                        batch_size=8,\n",
    "                        num_workers=4,\n",
    "                        pin_memory=True,\n",
    "                        shuffle=False, \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0d77ac-2261-4311-9381-6b465738e020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 11:02:06.942983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-19 11:02:06.950350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747641726.958814   35632 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747641726.961292   35632 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-19 11:02:06.970762: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from archisound import ArchiSound\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "autoencoder = ArchiSound.from_pretrained(\"dmae1d-ATC64-v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce172f84-80b8-4ab7-8046-80577e681ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig({\"image_size\": 128, \"audio_ctx_dim\": 32})\n",
    "\n",
    "# Инициализация\n",
    "diffusion = Diffusion(timesteps=1000, image_size=128, device=device)\n",
    "model = UNetWithCrossAttention(config).to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b17ff-7275-46a5-9e04-fafb7253dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e85b14516847d5828fd4c083358476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "unconditional_prob = 0.08\n",
    "\n",
    "epoch = 4\n",
    "for _ in tqdm(range(epoch)):\n",
    "    epo_train_losses = []\n",
    "    epo_val_losses = []\n",
    "    model.train()\n",
    "    for audio, images in train_loader:\n",
    "\n",
    "        if torch.rand(1) < unconditional_prob:\n",
    "            audio_embeds = None\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                audio_embeds = autoencoder.encode(audio.to(device)) # [B, d_audio, seq_len] [64, 32, 431]\n",
    "                \n",
    "            audio_embeds = audio_embeds.permute(0, 2, 1)  # [B, seq_len, d_audio] [64, 431, 32]\n",
    "\n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epo_train_losses.append(loss.item())\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(sum(epo_train_losses)/len(epo_train_losses))\n",
    "    # валидация\n",
    "    model.eval()\n",
    "    for audio, images in val_loader:\n",
    "        with torch.no_grad():\n",
    "            audio_embeds = autoencoder.encode(audio.to(device))\n",
    "            \n",
    "            audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "\n",
    "            # audio_embeds = torch.zeros((8, 431, 32)).to(device)\n",
    "            images = images.to(device)\n",
    "            \n",
    "            loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "    \n",
    "            epo_val_losses.append(loss.item())\n",
    "\n",
    "    val_losses.append(sum(epo_val_losses)/len(epo_val_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a864a86-9131-407f-b4bb-176053a6f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edf337-680f-4177-ac93-82d915b06082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epo_train_losses, label='train')\n",
    "plt.plot(epo_train_losses, label='val')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6fe8c-7ea3-40cc-92b3-c783c89335ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,image = next(iter(val_loader))\n",
    "audio_embeds = autoencoder.encode(audio.to(device))\n",
    "audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "\n",
    "model.eval()\n",
    "generated_image = diffusion.reverse_process(\n",
    "    model,\n",
    "    audio_embeds,\n",
    "    guidance_scale=7.5,\n",
    "    batch_size=8,\n",
    "    use_ddim=True,\n",
    "    timesteps=100  #число шагов\n",
    ")\n",
    "\n",
    "# 16 384\n",
    "# 32 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4586a27-8f05-4a99-9034-c82af78d8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.permute(generated_image[0].cpu()*0.5+0.5, (1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b3866-a522-4b9f-8557-d14a86b1541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# датасет обходится за 25 мин, 40 сек\n",
    "# при num_workers = 8 обходится за ~6 мин 30 сек\n",
    "# 1300 сек на эпоху\n",
    "\n",
    "# при двух attention блоках средние потери падают до 0.54 на 11 эпохах\n",
    "# при единственном блоке потери не пробивали отсечку в 0.9\n",
    "\n",
    "# 3 attention блока дали 0.84 на 4 эпохах. При продолжении обучения к концу 7 эпохи потери не изменились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd26e3-70fe-4336-9a42-511492c464e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.get_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bedb2a0-a494-452b-8cc4-3c60c8dcf14d",
   "metadata": {},
   "source": [
    "# Пробуем другой кодировщик звука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e43705-b5aa-4272-980d-71bff2d7f37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 13:49:43.214079: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-23 13:49:43.221137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747997383.229471   29282 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747997383.232145   29282 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747997383.238766   29282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747997383.238773   29282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747997383.238773   29282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747997383.238774   29282 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-23 13:49:43.241139: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor, Wav2Vec2Config\n",
    "\n",
    "class AudioEncoder:\n",
    "    def __init__(self,\n",
    "                model_name,\n",
    "                device,\n",
    "                sample_rate = 48000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.device = device\n",
    "        \n",
    "        config = Wav2Vec2Config.from_pretrained(model_name)\n",
    "        config.apply_spec_augment = False\n",
    "        \n",
    "        self.model = Wav2Vec2Model.from_pretrained(model_name, config=config).to(device)\n",
    "        \n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "    def stereo_to_mono(self, audio_tensor):\n",
    "        return torch.mean(audio_tensor, dim=1, keepdim=True)\n",
    "        \n",
    "\n",
    "    def preprocess_audio(self, audio_mono):\n",
    "        if self.sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                orig_freq=self.sample_rate,\n",
    "                new_freq=16000\n",
    "            )\n",
    "            audio_mono = resampler(audio_mono.cpu())\n",
    "        return audio_mono.to(self.device)\n",
    "        \n",
    "\n",
    "    def get_wav2vec2_features(self, audio_stereo_tensor):\n",
    "        inputs = self.feature_extractor(\n",
    "            audio_stereo_tensor.squeeze(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "    \n",
    "        inputs['input_values'] = inputs['input_values'].squeeze().to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "        return last_hidden_state\n",
    "\n",
    "    def __call__(self, audio):\n",
    "        audio = self.stereo_to_mono(audio)\n",
    "        audio = self.preprocess_audio(audio)\n",
    "        features = self.get_wav2vec2_features(audio)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783a603e-c190-4906-a782-aa8e5b4d1627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "sample_rate = 48000\n",
    "\n",
    "encode = AudioEncoder(\n",
    "    model_name,\n",
    "    device,\n",
    "    sample_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d35fcc6-7bcc-49b7-8915-f8b8eaa76da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig({\"image_size\": 128, \"audio_ctx_dim\": 768})\n",
    "\n",
    "# Инициализация\n",
    "diffusion = Diffusion(timesteps=1000, image_size=128, device=device)\n",
    "model = UNetWithCrossAttention(config).to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb796d6-a866-47f9-acca-b685d8325b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a1a957e9d245189b468816bcb5113c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 25\u001b[0m     epo_train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     27\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(epo_train_losses)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(epo_train_losses))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "unconditional_prob = 0.08\n",
    "\n",
    "epoch = 4\n",
    "for _ in tqdm(range(epoch)):\n",
    "    epo_train_losses = []\n",
    "    epo_val_losses = []\n",
    "    model.train()\n",
    "    for audio, images in train_loader:\n",
    "\n",
    "        if torch.rand(1) < unconditional_prob:\n",
    "            audio_embeds = None\n",
    "        else:\n",
    "            audio_embeds = encode(audio.to(device))\n",
    "\n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epo_train_losses.append(loss.item())\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(sum(epo_train_losses)/len(epo_train_losses))\n",
    "    # валидация\n",
    "    model.eval()\n",
    "    for audio, images in val_loader:\n",
    "        with torch.no_grad():\n",
    "            audio_embeds = encode(audio.to(device))\n",
    "\n",
    "            # audio_embeds = torch.zeros((8, 431, 32)).to(device)\n",
    "            images = images.to(device)\n",
    "            \n",
    "            loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "    \n",
    "            epo_val_losses.append(loss.item())\n",
    "\n",
    "    val_losses.append(sum(epo_val_losses)/len(epo_val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7e315-eefa-4615-a5fe-6a0438845f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
