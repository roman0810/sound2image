{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92fb9b3-bbf8-418a-84c6-0987d046fea3",
   "metadata": {},
   "source": [
    "### To do list:\n",
    "#### 1. ~~Скачать датасет *VGGSound*. Ссылка: https://huggingface.co/datasets/Loie/VGGSound/tree/main~~\n",
    "#### 2. ~~Все видео-файлы заменить на случайный кадр из них~~\n",
    "#### 2.1 ~~Перевести wav файлы к частоте 48кГц~~\n",
    "#### 3. ~~Реализовать DataLoader wav->ipeg~~\n",
    "#### 4. ~~Встроить аудио-энкодер. Ссылка: https://github.com/archinetai/archisound~~\n",
    "#### 5. ~~Реализовать сценарий обучения и обучить модель~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b2550-b41e-4952-b575-83bc645fc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.config import ModelConfig\n",
    "from models.unet import UNetWithCrossAttention\n",
    "from models.diffusion import Diffusion\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.SoundDataset import SoundDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877fb987-318f-43a5-a189-b77e18dead79",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/images\"\n",
    "sound_path = \"data/sounds\"\n",
    "\n",
    "data = SoundDataset(image_path, sound_path)\n",
    "train_data, val_data = torch.utils.data.random_split(data, [197889-10000, 10000])\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size=192,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True, \n",
    "                          drop_last=True)\n",
    "                         \n",
    "val_loader = DataLoader(val_data, \n",
    "                        batch_size=8,\n",
    "                        num_workers=8,\n",
    "                        pin_memory=True,\n",
    "                        shuffle=False, \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0d77ac-2261-4311-9381-6b465738e020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 16:11:09.749679: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-16 16:11:09.764940: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747401069.776048   11039 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747401069.781178   11039 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-16 16:11:09.797002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from archisound import ArchiSound\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "autoencoder = ArchiSound.from_pretrained(\"dmae1d-ATC64-v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce172f84-80b8-4ab7-8046-80577e681ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig({\n",
    "                    \"image_size\": 128,\n",
    "                    \"audio_ctx_dim\": 32\n",
    "                    })\n",
    "\n",
    "# Инициализация\n",
    "diffusion = Diffusion(timesteps=1000, image_size=128, device=device)\n",
    "model = UNetWithCrossAttention(config).to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8b17ff-7275-46a5-9e04-fafb7253dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0843ca7d9f2f4081a3a6dc397e8eb688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.72 GiB. GPU 0 has a total capacity of 23.65 GiB of which 3.33 GiB is free. Including non-PyTorch memory, this process has 19.59 GiB memory in use. Of the allocated memory 9.30 GiB is allocated by PyTorch, and 9.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m         audio_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [B, d_audio, seq_len] [64, 32, 431]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     audio_embeds \u001b[38;5;241m=\u001b[39m audio_embeds\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, seq_len, d_audio]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/archinetai/dmae1d-ATC64-v2/3ffeea68d4c069777055fce9ac77bbb67eec1d68/dmae.py:46\u001b[0m, in \u001b[0;36mDMAE1d.encode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/audio_diffusion_pytorch/models.py:110\u001b[0m, in \u001b[0;36mDiffusionAE.encode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/audio_encoders_pytorch/modules.py:550\u001b[0m, in \u001b[0;36mME1d.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Tuple[Tensor, Any]]:  \u001b[38;5;66;03m# type: ignore # noqa\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     magnitude, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     magnitude \u001b[38;5;241m=\u001b[39m rearrange(magnitude, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb c f l -> b (c f) l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    552\u001b[0m     magnitude \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(magnitude) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_log \u001b[38;5;28;01melse\u001b[39;00m magnitude\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/audio_encoders_pytorch/modules.py:476\u001b[0m, in \u001b[0;36mSTFT.encode\u001b[0;34m(self, wave)\u001b[0m\n\u001b[1;32m    473\u001b[0m b \u001b[38;5;241m=\u001b[39m wave\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    474\u001b[0m wave \u001b[38;5;241m=\u001b[39m rearrange(wave, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb c t -> (b c) t\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 476\u001b[0m stft \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwave\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_complex:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;66;03m# Returns real and imaginary\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     stft_a, stft_b \u001b[38;5;241m=\u001b[39m stft\u001b[38;5;241m.\u001b[39mreal, stft\u001b[38;5;241m.\u001b[39mimag\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/functional.py:704\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.72 GiB. GPU 0 has a total capacity of 23.65 GiB of which 3.33 GiB is free. Including non-PyTorch memory, this process has 19.59 GiB memory in use. Of the allocated memory 9.30 GiB is allocated by PyTorch, and 9.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "unconditional_prob = 0.08\n",
    "\n",
    "epoch = 4\n",
    "for _ in tqdm(range(epoch)):\n",
    "    epo_train_losses = []\n",
    "    epo_val_losses = []\n",
    "    model.train()\n",
    "    for audio, images in train_loader:\n",
    "\n",
    "        if torch.rand(1) < unconditional_prob:\n",
    "            audio_embeds = None\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                audio_embeds = autoencoder.encode(audio.to(device)) # [B, d_audio, seq_len] [64, 32, 431]\n",
    "                \n",
    "            audio_embeds = audio_embeds.permute(0, 2, 1)  # [B, seq_len, d_audio]\n",
    "\n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epo_train_losses.append(loss.item())\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(sum(epo_train_losses)/len(epo_train_losses))\n",
    "    # валидация\n",
    "    model.eval()\n",
    "    for audio, images in val_loader:\n",
    "        with torch.no_grad():\n",
    "            audio_embeds = autoencoder.encode(audio.to(device))\n",
    "            \n",
    "            audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "\n",
    "            # audio_embeds = torch.zeros((8, 431, 32)).to(device)\n",
    "            images = images.to(device)\n",
    "            \n",
    "            loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "    \n",
    "            epo_val_losses.append(loss.item())\n",
    "\n",
    "    val_losses.append(sum(epo_val_losses)/len(epo_val_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a864a86-9131-407f-b4bb-176053a6f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edf337-680f-4177-ac93-82d915b06082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epo_train_losses, label='train')\n",
    "plt.plot(epo_train_losses, label='val')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6fe8c-7ea3-40cc-92b3-c783c89335ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,image = next(iter(val_loader))\n",
    "audio_embeds = autoencoder.encode(audio.to(device))\n",
    "audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "\n",
    "model.eval()\n",
    "generated_image = diffusion.reverse_process(\n",
    "    model,\n",
    "    audio_embeds,\n",
    "    guidance_scale=7.5,\n",
    "    batch_size=8,\n",
    "    use_ddim=True,\n",
    "    timesteps=100  #число шагов\n",
    ")\n",
    "\n",
    "# 16 384\n",
    "# 32 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4586a27-8f05-4a99-9034-c82af78d8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.permute(generated_image[0].cpu()*0.5+0.5, (1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b3866-a522-4b9f-8557-d14a86b1541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# датасет обходится за 25 мин, 40 сек\n",
    "# при num_workers = 8 обходится за ~6 мин 30 сек\n",
    "# 1300 сек на эпоху\n",
    "\n",
    "# при двух attention блоках средние потери падают до 0.54 на 11 эпохах\n",
    "# при единственном блоке потери не пробивали отсечку в 0.9\n",
    "\n",
    "# 3 attention блока дали 0.84 на 4 эпохах. При продолжении обучения к концу 7 эпохи потери не изменились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55809c2-6374-47f9-ac97-f8b454a97cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for el in model.mid_block.attn.parameters():\n",
    "#     print(\"Attention stats:\", el.mean().item(), el.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df18674-2f31-4b76-9646-ac263bc6d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"weights2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd26e3-70fe-4336-9a42-511492c464e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.get_lr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
