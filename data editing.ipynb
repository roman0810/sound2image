{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877b0bb6-ae52-441a-aa74-d9912fc8d1a6",
   "metadata": {},
   "source": [
    "# Преобразование видео в пары звук-картинка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d78dd6-e9f2-4198-b3d7-60359a28fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "import moviepy.editor as mp\n",
    "import cv2\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Настройка логгирования и предупреждений\n",
    "logging.getLogger('moviepy').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_random_frame(cap, max_attempts=5):\n",
    "    \"\"\"Более надежный способ получения случайного кадра\"\"\"\n",
    "    for _ in range(max_attempts):\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if frame_count <= 0:\n",
    "            return None\n",
    "            \n",
    "        random_frame = random.randint(0, frame_count - 1)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame)\n",
    "        \n",
    "        # Даем видео несколько попыток декодирования\n",
    "        for _ in range(3):\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                return frame\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_video(video_path, sounds_dir, images_dir):\n",
    "    try:\n",
    "        # Обработка аудио\n",
    "        audio_path = sounds_dir / f\"{video_path.stem}.wav\"\n",
    "        clip = mp.VideoFileClip(str(video_path))\n",
    "            \n",
    "        if clip.duration > 10:\n",
    "            audio = clip.subclip(0, 10).audio\n",
    "        else:\n",
    "            audio = clip.audio\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Обработка изображения с улучшенным чтением кадров\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        frame = get_random_frame(cap)\n",
    "        \n",
    "        if frame is not None:\n",
    "            resized = cv2.resize(frame, (128, 128))\n",
    "            img_path = images_dir / f\"{video_path.stem}.jpg\"\n",
    "            cv2.imwrite(str(img_path), resized)\n",
    "\n",
    "            audio.write_audiofile(str(audio_path), fps=44100, codec='pcm_s16le', verbose=False, logger=None)\n",
    "        else:\n",
    "            print(f\"Не удалось декодировать файл {video_path}\")\n",
    "        \n",
    "        cap.release()\n",
    "        clip.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке {video_path}: {str(e)}\")\n",
    "\n",
    "def process_all_videos(root_dir):\n",
    "    root_path = Path(root_dir)\n",
    "    sounds_dir = root_path / \"sounds\"\n",
    "    images_dir = root_path / \"images\"\n",
    "\n",
    "    root_path = root_path / \"videos\"\n",
    "    \n",
    "    sounds_dir.mkdir(exist_ok=True)\n",
    "    images_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    video_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith('.mp4'):\n",
    "                video_files.append(Path(dirpath) / filename)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_video, video_path, sounds_dir, images_dir) \n",
    "                 for video_path in video_files]\n",
    "        concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcdf2c5-8485-4130-9792-3a54996b89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"data\"\n",
    "# process_all_videos(root_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09e01b-fa62-4593-b41e-b3740908ea90",
   "metadata": {},
   "source": [
    "# Преобразование звука в записанные эмбединги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef30f93d-b001-4f1c-b2c9-1ae403811041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usr/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "from torchvision.io import read_image\n",
    "import torchaudio\n",
    "import torch\n",
    "from models.AudioEncoder import AudioEncoder\n",
    "\n",
    "\n",
    "class SoundDataset(Dataset):\n",
    "    def __init__(self , image_path, sound_path):\n",
    "        names = []\n",
    "        for path in glob.glob(f'{image_path}/*.jpg'):\n",
    "            name = path.split('/')[-1][:-4]\n",
    "            names.append(name)\n",
    "\n",
    "        self.names = names\n",
    "        self.im_path = image_path\n",
    "        self.au_path = sound_path\n",
    "        self.stanart_len = 480000\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self , index):\n",
    "        audio_path = f\"{self.au_path}/{self.names[index]}.wav\"\n",
    "        waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "        # необходим стерео звук, если он одноканальный то приводим к стерео\n",
    "        if waveform.shape[0] == 1:\n",
    "            stereo = torch.zeros((2, waveform.shape[1]), dtype=torch.float)\n",
    "            stereo[0] = waveform[0]\n",
    "            stereo[1] = waveform[0]\n",
    "            waveform = stereo\n",
    "\n",
    "        elif waveform.shape[0] != 2:\n",
    "            raise ValueError(f\"audio {self.names[index]} must be stereo or mono, but {waveform.shape[0]} channels were given\")\n",
    "\n",
    "        # все тензоры должны быть стандартного размера (только для обучения)\n",
    "        if waveform.shape[1] < self.stanart_len:\n",
    "            ext_waveform = torch.zeros((2, self.stanart_len), dtype=torch.float)\n",
    "            ext_waveform[:, :waveform.shape[1]] = waveform\n",
    "            waveform = ext_waveform\n",
    "        elif waveform.shape[1] > self.stanart_len:\n",
    "            waveform = waveform[:, :self.stanart_len]\n",
    "\n",
    "        return waveform.float(), self.names[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b24aa6-0749-4ae9-ad09-b0a9dbf580eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "image_path = \"data/images\"\n",
    "sound_path = \"data/sounds\"\n",
    "\n",
    "data = SoundDataset(image_path, sound_path)\n",
    "loader = DataLoader(data, 128)\n",
    "\n",
    "encode = AudioEncoder(\"facebook/wav2vec2-base-960h\",\n",
    "                    torch.device(f'cuda:{0}'),\n",
    "                    48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12cae1fc-f1a0-4731-9253-ddcbc40e32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def write_tensors(loader, encode):\n",
    "    loader = iter(loader)\n",
    "    \n",
    "    \n",
    "    # Параметры\n",
    "    TENSOR_SHAPE = (499, 768)\n",
    "    STRING_LENGTH = 32\n",
    "    CHUNK_SIZE = 128\n",
    "    TOTAL_RECORDS = len(data)\n",
    "    \n",
    "    # Определяем структурированный тип данных\n",
    "    dt = np.dtype([\n",
    "        ('tensor', np.float32, TENSOR_SHAPE),\n",
    "        ('metadata', f'S{STRING_LENGTH}')  # Строка фиксированной длины\n",
    "    ])\n",
    "    \n",
    "    # Создаем HDF5 файл и датасет\n",
    "    with h5py.File('data/embeds/sound_embeds.h5', 'w') as f:\n",
    "        dset = f.create_dataset(\n",
    "            'data',\n",
    "            shape=(0,),\n",
    "            maxshape=(None,),\n",
    "            dtype=dt,\n",
    "            chunks=(CHUNK_SIZE,)  # Размер чанка = размеру пакета\n",
    "        )\n",
    "    \n",
    "        # Цикл записи по пакетам\n",
    "        for i in range(0, TOTAL_RECORDS, CHUNK_SIZE):\n",
    "            num_to_write = min(CHUNK_SIZE, TOTAL_RECORDS - i)\n",
    "\n",
    "            sound, metadata_strings = next(loader)\n",
    "            tensors = encode(sound.cuda()).cpu().numpy()\n",
    "    \n",
    "            # Подготавливаем буфер для записи\n",
    "            buffer = np.empty(num_to_write, dtype=dt)\n",
    "            for k in range(num_to_write):\n",
    "                buffer[k]['tensor'] = tensors[k]\n",
    "                buffer[k]['metadata'] = metadata_strings[k].encode('utf-8')\n",
    "    \n",
    "            # Расширяем датасет и записываем данные\n",
    "            dset.resize(dset.shape[0] + num_to_write, axis=0)\n",
    "            dset[-num_to_write:] = buffer\n",
    "\n",
    "            print(f\"\\rWritten rows: {i+CHUNK_SIZE} out of {TOTAL_RECORDS}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b311e8b-ab75-4fc3-a5aa-7f2d8a140bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written rows: 197888 out of 197889"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [512, 1, 10], expected input[1, 160000, 1] to have 1 channels, but got 160000 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mwrite_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mwrite_tensors\u001b[39m\u001b[34m(loader, encode)\u001b[39m\n\u001b[32m     32\u001b[39m num_to_write = \u001b[38;5;28mmin\u001b[39m(CHUNK_SIZE, TOTAL_RECORDS - i)\n\u001b[32m     34\u001b[39m sound, metadata_strings = \u001b[38;5;28mnext\u001b[39m(loader)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m tensors = \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.cpu().numpy()\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Подготавливаем буфер для записи\u001b[39;00m\n\u001b[32m     38\u001b[39m buffer = np.empty(num_to_write, dtype=dt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/sound2image/models/AudioEncoder.py:53\u001b[39m, in \u001b[36mAudioEncoder.__call__\u001b[39m\u001b[34m(self, audio)\u001b[39m\n\u001b[32m     51\u001b[39m audio = \u001b[38;5;28mself\u001b[39m.stereo_to_mono(audio)\n\u001b[32m     52\u001b[39m audio = \u001b[38;5;28mself\u001b[39m.preprocess_audio(audio)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_wav2vec2_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/sound2image/models/AudioEncoder.py:45\u001b[39m, in \u001b[36mAudioEncoder.get_wav2vec2_features\u001b[39m\u001b[34m(self, audio_stereo_tensor)\u001b[39m\n\u001b[32m     42\u001b[39m inputs[\u001b[33m'\u001b[39m\u001b[33minput_values\u001b[39m\u001b[33m'\u001b[39m] = inputs[\u001b[33m'\u001b[39m\u001b[33minput_values\u001b[39m\u001b[33m'\u001b[39m].squeeze().to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m last_hidden_state = outputs.last_hidden_state  \u001b[38;5;66;03m# (batch_size, seq_len, hidden_size)\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m last_hidden_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1714\u001b[39m, in \u001b[36mWav2Vec2Model.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1709\u001b[39m output_hidden_states = (\n\u001b[32m   1710\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1711\u001b[39m )\n\u001b[32m   1712\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m extract_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m extract_features = extract_features.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m   1717\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1718\u001b[39m     \u001b[38;5;66;03m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:437\u001b[39m, in \u001b[36mWav2Vec2FeatureEncoder.forward\u001b[39m\u001b[34m(self, input_values)\u001b[39m\n\u001b[32m    432\u001b[39m         hidden_states = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    433\u001b[39m             conv_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    434\u001b[39m             hidden_states,\n\u001b[32m    435\u001b[39m         )\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m         hidden_states = \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:334\u001b[39m, in \u001b[36mWav2Vec2GroupNormConvLayer.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m    336\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.activation(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:375\u001b[39m, in \u001b[36mConv1d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:370\u001b[39m, in \u001b[36mConv1d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv1d(\n\u001b[32m    360\u001b[39m         F.pad(\n\u001b[32m    361\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    369\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [512, 1, 10], expected input[1, 160000, 1] to have 1 channels, but got 160000 channels instead"
     ]
    }
   ],
   "source": [
    "write_tensors(loader, encode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
