{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92fb9b3-bbf8-418a-84c6-0987d046fea3",
   "metadata": {},
   "source": [
    "### To do list:\n",
    "#### 1. ~~Скачать датасет *VGGSound*. Ссылка: https://huggingface.co/datasets/Loie/VGGSound/tree/main~~\n",
    "#### 2. ~~Все видео-файлы заменить на случайный кадр из них~~\n",
    "#### 2.1 ~~Перевести wav файлы к частоте 48кГц~~\n",
    "#### 3. ~~Реализовать DataLoader wav->ipeg~~\n",
    "#### 4. ~~Встроить аудио-энкодер. Ссылка: https://github.com/archinetai/archisound~~\n",
    "#### 5. ~~Реализовать сценарий обучения и обучить модель~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b2550-b41e-4952-b575-83bc645fc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.config import ModelConfig\n",
    "from models.unet import UNetWithCrossAttention\n",
    "from models.diffusion import Diffusion\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.SoundDataset import SoundDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877fb987-318f-43a5-a189-b77e18dead79",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/images\"\n",
    "sound_path = \"data/sounds\"\n",
    "\n",
    "data = SoundDataset(image_path, sound_path)\n",
    "train_data, val_data = torch.utils.data.random_split(data, [197889-10000, 10000])\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size=128,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True, \n",
    "                          drop_last=True)\n",
    "                         \n",
    "val_loader = DataLoader(val_data, \n",
    "                        batch_size=8,\n",
    "                        num_workers=8,\n",
    "                        pin_memory=True,\n",
    "                        shuffle=False, \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0d77ac-2261-4311-9381-6b465738e020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 11:02:06.942983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-19 11:02:06.950350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747641726.958814   35632 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747641726.961292   35632 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-19 11:02:06.970762: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from archisound import ArchiSound\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "autoencoder = ArchiSound.from_pretrained(\"dmae1d-ATC64-v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce172f84-80b8-4ab7-8046-80577e681ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig({\"image_size\": 128, \"audio_ctx_dim\": 32})\n",
    "\n",
    "# Инициализация\n",
    "diffusion = Diffusion(timesteps=1000, image_size=128, device=device)\n",
    "model = UNetWithCrossAttention(config).to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b17ff-7275-46a5-9e04-fafb7253dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e85b14516847d5828fd4c083358476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "unconditional_prob = 0.08\n",
    "\n",
    "epoch = 4\n",
    "for _ in tqdm(range(epoch)):\n",
    "    epo_train_losses = []\n",
    "    epo_val_losses = []\n",
    "    model.train()\n",
    "    for audio, images in train_loader:\n",
    "\n",
    "        if torch.rand(1) < unconditional_prob:\n",
    "            audio_embeds = None\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                audio_embeds = autoencoder.encode(audio.to(device)) # [B, d_audio, seq_len] [64, 32, 431]\n",
    "                \n",
    "            audio_embeds = audio_embeds.permute(0, 2, 1)  # [B, seq_len, d_audio]\n",
    "\n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epo_train_losses.append(loss.item())\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(sum(epo_train_losses)/len(epo_train_losses))\n",
    "    # валидация\n",
    "    model.eval()\n",
    "    for audio, images in val_loader:\n",
    "        with torch.no_grad():\n",
    "            audio_embeds = autoencoder.encode(audio.to(device))\n",
    "            \n",
    "            audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "\n",
    "            # audio_embeds = torch.zeros((8, 431, 32)).to(device)\n",
    "            images = images.to(device)\n",
    "            \n",
    "            loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "    \n",
    "            epo_val_losses.append(loss.item())\n",
    "\n",
    "    val_losses.append(sum(epo_val_losses)/len(epo_val_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a864a86-9131-407f-b4bb-176053a6f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edf337-680f-4177-ac93-82d915b06082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epo_train_losses, label='train')\n",
    "plt.plot(epo_train_losses, label='val')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6fe8c-7ea3-40cc-92b3-c783c89335ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,image = next(iter(val_loader))\n",
    "audio_embeds = autoencoder.encode(audio.to(device))\n",
    "audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "\n",
    "model.eval()\n",
    "generated_image = diffusion.reverse_process(\n",
    "    model,\n",
    "    audio_embeds,\n",
    "    guidance_scale=7.5,\n",
    "    batch_size=8,\n",
    "    use_ddim=True,\n",
    "    timesteps=100  #число шагов\n",
    ")\n",
    "\n",
    "# 16 384\n",
    "# 32 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4586a27-8f05-4a99-9034-c82af78d8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.permute(generated_image[0].cpu()*0.5+0.5, (1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b3866-a522-4b9f-8557-d14a86b1541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# датасет обходится за 25 мин, 40 сек\n",
    "# при num_workers = 8 обходится за ~6 мин 30 сек\n",
    "# 1300 сек на эпоху\n",
    "\n",
    "# при двух attention блоках средние потери падают до 0.54 на 11 эпохах\n",
    "# при единственном блоке потери не пробивали отсечку в 0.9\n",
    "\n",
    "# 3 attention блока дали 0.84 на 4 эпохах. При продолжении обучения к концу 7 эпохи потери не изменились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55809c2-6374-47f9-ac97-f8b454a97cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for el in model.mid_block.attn.parameters():\n",
    "#     print(\"Attention stats:\", el.mean().item(), el.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df18674-2f31-4b76-9646-ac263bc6d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"weights2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd26e3-70fe-4336-9a42-511492c464e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e400c458-a24c-47a0-bf60-d2b23c6554a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e1f6fb-b669-4f0b-aef9-993f31de41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | down_blocks.0.conv1.weight: torch.Size([32, 3, 3, 3])\n",
      "1 | down_blocks.0.conv1.bias: torch.Size([32])\n",
      "2 | down_blocks.0.conv2.weight: torch.Size([32, 32, 3, 3])\n",
      "3 | down_blocks.0.conv2.bias: torch.Size([32])\n",
      "4 | down_blocks.0.BN1.weight: torch.Size([32])\n",
      "5 | down_blocks.0.BN1.bias: torch.Size([32])\n",
      "6 | down_blocks.0.downsample.weight: torch.Size([32, 32, 3, 3])\n",
      "7 | down_blocks.0.downsample.bias: torch.Size([32])\n",
      "8 | down_blocks.0.time_embed.0.weight: torch.Size([32, 1])\n",
      "9 | down_blocks.0.time_embed.0.bias: torch.Size([32])\n",
      "10 | down_blocks.0.time_embed.2.weight: torch.Size([32, 32])\n",
      "11 | down_blocks.0.time_embed.2.bias: torch.Size([32])\n",
      "12 | down_blocks.1.conv1.weight: torch.Size([64, 32, 3, 3])\n",
      "13 | down_blocks.1.conv1.bias: torch.Size([64])\n",
      "14 | down_blocks.1.conv2.weight: torch.Size([64, 64, 3, 3])\n",
      "15 | down_blocks.1.conv2.bias: torch.Size([64])\n",
      "16 | down_blocks.1.BN1.weight: torch.Size([64])\n",
      "17 | down_blocks.1.BN1.bias: torch.Size([64])\n",
      "18 | down_blocks.1.downsample.weight: torch.Size([64, 64, 3, 3])\n",
      "19 | down_blocks.1.downsample.bias: torch.Size([64])\n",
      "20 | down_blocks.1.time_embed.0.weight: torch.Size([64, 1])\n",
      "21 | down_blocks.1.time_embed.0.bias: torch.Size([64])\n",
      "22 | down_blocks.1.time_embed.2.weight: torch.Size([64, 64])\n",
      "23 | down_blocks.1.time_embed.2.bias: torch.Size([64])\n",
      "24 | down_blocks.2.conv1.weight: torch.Size([128, 64, 3, 3])\n",
      "25 | down_blocks.2.conv1.bias: torch.Size([128])\n",
      "26 | down_blocks.2.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "27 | down_blocks.2.conv2.bias: torch.Size([128])\n",
      "28 | down_blocks.2.BN1.weight: torch.Size([128])\n",
      "29 | down_blocks.2.BN1.bias: torch.Size([128])\n",
      "30 | down_blocks.2.downsample.weight: torch.Size([128, 128, 3, 3])\n",
      "31 | down_blocks.2.downsample.bias: torch.Size([128])\n",
      "32 | down_blocks.2.time_embed.0.weight: torch.Size([128, 1])\n",
      "33 | down_blocks.2.time_embed.0.bias: torch.Size([128])\n",
      "34 | down_blocks.2.time_embed.2.weight: torch.Size([128, 128])\n",
      "35 | down_blocks.2.time_embed.2.bias: torch.Size([128])\n",
      "36 | down_blocks.3.conv1.weight: torch.Size([256, 128, 3, 3])\n",
      "37 | down_blocks.3.conv1.bias: torch.Size([256])\n",
      "38 | down_blocks.3.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "39 | down_blocks.3.conv2.bias: torch.Size([256])\n",
      "40 | down_blocks.3.BN1.weight: torch.Size([256])\n",
      "41 | down_blocks.3.BN1.bias: torch.Size([256])\n",
      "42 | down_blocks.3.downsample.weight: torch.Size([256, 256, 3, 3])\n",
      "43 | down_blocks.3.downsample.bias: torch.Size([256])\n",
      "44 | down_blocks.3.time_embed.0.weight: torch.Size([256, 1])\n",
      "45 | down_blocks.3.time_embed.0.bias: torch.Size([256])\n",
      "46 | down_blocks.3.time_embed.2.weight: torch.Size([256, 256])\n",
      "47 | down_blocks.3.time_embed.2.bias: torch.Size([256])\n",
      "48 | mid_block_half.conv1.weight: torch.Size([128, 128, 3, 3])\n",
      "49 | mid_block_half.conv1.bias: torch.Size([128])\n",
      "50 | mid_block_half.BN1.weight: torch.Size([128])\n",
      "51 | mid_block_half.BN1.bias: torch.Size([128])\n",
      "52 | mid_block_half.attn.W_Q.weight: torch.Size([128, 128])\n",
      "53 | mid_block_half.attn.W_Q.bias: torch.Size([128])\n",
      "54 | mid_block_half.attn.W_K.weight: torch.Size([128, 32])\n",
      "55 | mid_block_half.attn.W_K.bias: torch.Size([128])\n",
      "56 | mid_block_half.attn.W_V.weight: torch.Size([128, 32])\n",
      "57 | mid_block_half.attn.W_V.bias: torch.Size([128])\n",
      "58 | mid_block_half.BN2.weight: torch.Size([128])\n",
      "59 | mid_block_half.BN2.bias: torch.Size([128])\n",
      "60 | mid_block_half.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "61 | mid_block_half.conv2.bias: torch.Size([128])\n",
      "62 | mid_block_bot.conv1.weight: torch.Size([256, 256, 3, 3])\n",
      "63 | mid_block_bot.conv1.bias: torch.Size([256])\n",
      "64 | mid_block_bot.BN1.weight: torch.Size([256])\n",
      "65 | mid_block_bot.BN1.bias: torch.Size([256])\n",
      "66 | mid_block_bot.attn.W_Q.weight: torch.Size([256, 256])\n",
      "67 | mid_block_bot.attn.W_Q.bias: torch.Size([256])\n",
      "68 | mid_block_bot.attn.W_K.weight: torch.Size([256, 32])\n",
      "69 | mid_block_bot.attn.W_K.bias: torch.Size([256])\n",
      "70 | mid_block_bot.attn.W_V.weight: torch.Size([256, 32])\n",
      "71 | mid_block_bot.attn.W_V.bias: torch.Size([256])\n",
      "72 | mid_block_bot.BN2.weight: torch.Size([256])\n",
      "73 | mid_block_bot.BN2.bias: torch.Size([256])\n",
      "74 | mid_block_bot.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "75 | mid_block_bot.conv2.bias: torch.Size([256])\n",
      "76 | up_blocks.0.upsample.weight: torch.Size([512, 512, 3, 3])\n",
      "77 | up_blocks.0.upsample.bias: torch.Size([512])\n",
      "78 | up_blocks.0.conv1.weight: torch.Size([128, 512, 3, 3])\n",
      "79 | up_blocks.0.conv1.bias: torch.Size([128])\n",
      "80 | up_blocks.0.BN1.weight: torch.Size([128])\n",
      "81 | up_blocks.0.BN1.bias: torch.Size([128])\n",
      "82 | up_blocks.0.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "83 | up_blocks.0.conv2.bias: torch.Size([128])\n",
      "84 | up_blocks.0.time_embed.0.weight: torch.Size([128, 1])\n",
      "85 | up_blocks.0.time_embed.0.bias: torch.Size([128])\n",
      "86 | up_blocks.0.time_embed.2.weight: torch.Size([128, 128])\n",
      "87 | up_blocks.0.time_embed.2.bias: torch.Size([128])\n",
      "88 | up_blocks.1.upsample.weight: torch.Size([256, 256, 3, 3])\n",
      "89 | up_blocks.1.upsample.bias: torch.Size([256])\n",
      "90 | up_blocks.1.conv1.weight: torch.Size([64, 256, 3, 3])\n",
      "91 | up_blocks.1.conv1.bias: torch.Size([64])\n",
      "92 | up_blocks.1.BN1.weight: torch.Size([64])\n",
      "93 | up_blocks.1.BN1.bias: torch.Size([64])\n",
      "94 | up_blocks.1.conv2.weight: torch.Size([64, 64, 3, 3])\n",
      "95 | up_blocks.1.conv2.bias: torch.Size([64])\n",
      "96 | up_blocks.1.time_embed.0.weight: torch.Size([64, 1])\n",
      "97 | up_blocks.1.time_embed.0.bias: torch.Size([64])\n",
      "98 | up_blocks.1.time_embed.2.weight: torch.Size([64, 64])\n",
      "99 | up_blocks.1.time_embed.2.bias: torch.Size([64])\n",
      "100 | up_blocks.2.upsample.weight: torch.Size([128, 128, 3, 3])\n",
      "101 | up_blocks.2.upsample.bias: torch.Size([128])\n",
      "102 | up_blocks.2.conv1.weight: torch.Size([32, 128, 3, 3])\n",
      "103 | up_blocks.2.conv1.bias: torch.Size([32])\n",
      "104 | up_blocks.2.BN1.weight: torch.Size([32])\n",
      "105 | up_blocks.2.BN1.bias: torch.Size([32])\n",
      "106 | up_blocks.2.conv2.weight: torch.Size([32, 32, 3, 3])\n",
      "107 | up_blocks.2.conv2.bias: torch.Size([32])\n",
      "108 | up_blocks.2.time_embed.0.weight: torch.Size([32, 1])\n",
      "109 | up_blocks.2.time_embed.0.bias: torch.Size([32])\n",
      "110 | up_blocks.2.time_embed.2.weight: torch.Size([32, 32])\n",
      "111 | up_blocks.2.time_embed.2.bias: torch.Size([32])\n",
      "112 | up_blocks.3.upsample.weight: torch.Size([64, 64, 3, 3])\n",
      "113 | up_blocks.3.upsample.bias: torch.Size([64])\n",
      "114 | up_blocks.3.conv1.weight: torch.Size([3, 64, 3, 3])\n",
      "115 | up_blocks.3.conv1.bias: torch.Size([3])\n",
      "116 | up_blocks.3.BN1.weight: torch.Size([3])\n",
      "117 | up_blocks.3.BN1.bias: torch.Size([3])\n",
      "118 | up_blocks.3.conv2.weight: torch.Size([3, 3, 3, 3])\n",
      "119 | up_blocks.3.conv2.bias: torch.Size([3])\n",
      "120 | up_blocks.3.time_embed.0.weight: torch.Size([3, 1])\n",
      "121 | up_blocks.3.time_embed.0.bias: torch.Size([3])\n",
      "122 | up_blocks.3.time_embed.2.weight: torch.Size([3, 3])\n",
      "123 | up_blocks.3.time_embed.2.bias: torch.Size([3])\n",
      "124 | norm.weight: torch.Size([3])\n",
      "125 | norm.bias: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    print(f\"{i} | {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911ca3cb-58f4-46f1-8632-336a88c35731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 17:53:24.233455: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-22 17:53:24.241455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747925604.250008  108046 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747925604.252504  108046 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747925604.259660  108046 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747925604.259668  108046 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747925604.259669  108046 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747925604.259669  108046 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-22 17:53:24.262039: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", \n",
    "                                      torch_dtype=torch.float16, \n",
    "                                      attn_implementation=\"flash_attention_2\").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa315fa-a2d4-45e1-8feb-676b3ff4d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10000])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.rand((2,10000)).half().cuda()\n",
    "\n",
    "print(sample.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e41165-e237-43ee-86ee-3ab9476507fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fa56edc-a446-4e3a-a042-a3bff41a500b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fa535b5-7671-436f-b947-835dae8e5c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c768e199-f873-4870-8480-63fbace4b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность признакового вектора: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# # Предположим, у вас есть стерео тензор audio_tensor формы (2, num_samples)\n",
    "# def stereo_to_mono(audio_tensor):\n",
    "#     # Усредняем оба канала (можно также выбрать один канал)\n",
    "#     return torch.mean(audio_tensor, dim=0, keepdim=True)\n",
    "\n",
    "# def preprocess_audio(audio_mono, target_sample_rate=16000):\n",
    "#     # Если частота дискретизации не 16 кГц, ресемплируем\n",
    "#     if target_sample_rate != 16000:\n",
    "#         resampler = torchaudio.transforms.Resample(\n",
    "#             orig_freq=target_sample_rate,\n",
    "#             new_freq=16000\n",
    "#         )\n",
    "#         audio_mono = resampler(audio_mono)\n",
    "#     return audio_mono\n",
    "\n",
    "# Загружаем модель и feature extractor (можно заменить на другую версию)\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "def get_wav2vec2_features(audio_stereo_tensor, original_sample_rate):\n",
    "    # 1. Конвертируем стерео в моно\n",
    "    # audio_mono = stereo_to_mono(audio_stereo_tensor)\n",
    "    \n",
    "    # # 2. Ресемплируем до 16 кГц если нужно\n",
    "    # audio_preprocessed = preprocess_audio(audio_mono, original_sample_rate)\n",
    "    \n",
    "    # 3. Нормализуем и извлекаем features\n",
    "    inputs = feature_extractor(\n",
    "        audio_stereo_tensor.squeeze().numpy(),\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # 4. Получаем скрытые состояния\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 5. Возвращаем усреднённые features по времени (можно использовать и другие агрегации)\n",
    "    last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "    return torch.mean(last_hidden_state, dim=1)    # (batch_size, hidden_size)\n",
    "\n",
    "# Пример использования:\n",
    "# Предположим, у нас есть стерео тензор (2 канала, 44100 Гц)\n",
    "stereo_audio = torch.randn(1, 44100 * 100)  # 3 секунды аудио\n",
    "sample_rate = 44100\n",
    "\n",
    "features = get_wav2vec2_features(stereo_audio, sample_rate)\n",
    "print(f\"Размерность признакового вектора: {features.shape}\")  # Ожидаем [1, 768] для wav2vec2-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e43705-b5aa-4272-980d-71bff2d7f37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
