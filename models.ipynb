{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92fb9b3-bbf8-418a-84c6-0987d046fea3",
   "metadata": {},
   "source": [
    "### To do list:\n",
    "#### 1. ~~Скачать датасет *VGGSound*. Ссылка: https://huggingface.co/datasets/Loie/VGGSound/tree/main~~\n",
    "#### 2. ~~Все видео-файлы заменить на случайный кадр из них~~\n",
    "#### 2.1 ~~Перевести wav файлы к частоте 48кГц~~\n",
    "#### 3. ~~Реализовать DataLoader wav->ipeg~~\n",
    "#### 4. ~~Встроить аудио-энкодер. Ссылка: https://github.com/archinetai/archisound~~\n",
    "#### 5. ~~Реализовать сценарий обучения и обучить модель~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b2550-b41e-4952-b575-83bc645fc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.config import ModelConfig\n",
    "from models.unet import UNetWithCrossAttention\n",
    "from models.diffusion import Diffusion\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665cca1c-4b0a-4a8d-930f-03334109f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "from torchvision.io import read_image\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "class SoundDataset(Dataset):\n",
    "    def __init__(self , image_path, sound_path):\n",
    "        names = []\n",
    "        for path in glob.glob(f'{image_path}/*.jpg'):\n",
    "            name = path.split('/')[-1][:-4]\n",
    "            names.append(name)\n",
    "\n",
    "        self.names = names\n",
    "        self.im_path = image_path\n",
    "        self.au_path = sound_path\n",
    "        self.stanart_len = 441000\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self , index):\n",
    "        image = read_image(f\"{self.im_path}/{self.names[index]}.jpg\")/255.0\n",
    "        \n",
    "        audio_path = f\"{self.au_path}/{self.names[index]}.wav\"\n",
    "        waveform, _ = torchaudio.load(audio_path)\n",
    "        \n",
    "        # необходим стерео звук, если он одноканальный то приводим к стерео\n",
    "        if waveform.shape[0] == 1:\n",
    "            stereo = torch.zeros((2, waveform.shape[1]), dtype=torch.float)\n",
    "            stereo[0] = waveform[0]\n",
    "            stereo[1] = waveform[0]\n",
    "            waveform = stereo\n",
    "\n",
    "        elif waveform.shape[0] != 2:\n",
    "            raise ValueError(f\"audio {self.names[index]} must be stereo or mono, but {waveform.shape[0]} channels were given\")\n",
    "\n",
    "        # все тензоры должны быть стандартного размера (только для обучения)\n",
    "        if waveform.shape[1] < self.stanart_len:\n",
    "            ext_waveform = torch.zeros((2, self.stanart_len), dtype=torch.float)\n",
    "            ext_waveform[:, :waveform.shape[1]] = waveform\n",
    "            waveform = ext_waveform\n",
    "        elif waveform.shape[1] > self.stanart_len:\n",
    "            waveform = waveform[:, :self.stanart_len]\n",
    "        \n",
    "        return waveform.float(), image.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877fb987-318f-43a5-a189-b77e18dead79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_path = \"data/images\"\n",
    "sound_path = \"data/sounds\"\n",
    "\n",
    "data = SoundDataset(image_path, sound_path)\n",
    "train_data, val_data = torch.utils.data.random_split(data, [197889-10000, 10000])\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size=64,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True, \n",
    "                          drop_last=True)\n",
    "                         \n",
    "val_loader = DataLoader(val_data, \n",
    "                        batch_size=8,\n",
    "                        num_workers=8,\n",
    "                        pin_memory=True,\n",
    "                        shuffle=False, \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0d77ac-2261-4311-9381-6b465738e020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 20:25:37.351518: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-07 20:25:37.359633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746638737.368425  317065 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746638737.371001  317065 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-07 20:25:37.380732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from archisound import ArchiSound\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "autoencoder = ArchiSound.from_pretrained(\"dmae1d-ATC64-v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce172f84-80b8-4ab7-8046-80577e681ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig({\n",
    "                    \"image_size\": 128,\n",
    "                    \"audio_ctx_dim\": 32\n",
    "                    })\n",
    "\n",
    "# Инициализация\n",
    "diffusion = Diffusion(timesteps=1000, image_size=128, device=device)\n",
    "model = UNetWithCrossAttention(config).to(device)\n",
    "\n",
    "# Обучение\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8b17ff-7275-46a5-9e04-fafb7253dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a3c554dda44c8c995e85aa915a2458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 59, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 35, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/usr/miniforge3/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio, images \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m         audio_embeds \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mencode(\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# [B, d_audio, seq_len]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     audio_embeds \u001b[38;5;241m=\u001b[39m audio_embeds\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, seq_len, d_audio]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "for _ in tqdm(range(epoch)):\n",
    "    epo_train_losses = []\n",
    "    epo_val_losses = []\n",
    "    for audio, images in train_loader:\n",
    "        with torch.no_grad():\n",
    "            audio_embeds = autoencoder.encode(audio.to(device)) # [B, d_audio, seq_len]\n",
    "            \n",
    "        audio_embeds = audio_embeds.permute(0, 2, 1)  # [B, seq_len, d_audio]\n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epo_train_losses.append(loss.item())\n",
    "\n",
    "    train_losses.append(sum(epo_train_losses)/len(epo_train_losses))\n",
    "    # валидация\n",
    "    for audio, images in val_loader:\n",
    "        with torch.no_grad():\n",
    "            audio_embeds = autoencoder.encode(audio.to(device))\n",
    "            \n",
    "            audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "            images = images.to(device)\n",
    "            \n",
    "            loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "    \n",
    "            epo_val_losses.append(loss.item())\n",
    "\n",
    "    val_losses.append(sum(epo_val_losses)/len(epo_val_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a864a86-9131-407f-b4bb-176053a6f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6fe8c-7ea3-40cc-92b3-c783c89335ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,image = next(iter(val_loader))\n",
    "audio_embeds = autoencoder.encode(audio.to(device))\n",
    "audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "\n",
    "generated_image = diffusion.reverse_process(\n",
    "    model,\n",
    "    audio_embeds,\n",
    "    batch_size=8,\n",
    "    use_ddim=True,\n",
    "    timesteps= 50  # во сколько раз сокращаем число шагов\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4586a27-8f05-4a99-9034-c82af78d8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.permute(generated_image[0].cpu(), (1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b3866-a522-4b9f-8557-d14a86b1541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# датасет обходится за 25 мин, 40 сек\n",
    "# при num_workers = 8 обходится за ~6 мин 30 сек\n",
    "# 1300 сек на эпоху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e677e3-5636-4440-abbb-ee356d82e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
    "test = torch.rand((8,3,16,16))\n",
    "\n",
    "upsample(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477cc6b-2b82-4cf0-bb21-21ed4f3b5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample = torch.nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "upsample(test, output_size=(-1, 3, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25a2a4-ea43-47af-bd07-f897a5f8312b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
