{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92fb9b3-bbf8-418a-84c6-0987d046fea3",
   "metadata": {},
   "source": [
    "### To do list:\n",
    "#### 1. ~~Скачать датасет *VGGSound*. Ссылка: https://huggingface.co/datasets/Loie/VGGSound/tree/main~~\n",
    "#### 2. ~~Все видео-файлы заменить на случайный кадр из них~~\n",
    "#### 2.1 ~~Перевести wav файлы к частоте 48кГц~~\n",
    "#### 3. ~~Реализовать DataLoader wav->ipeg~~\n",
    "#### 4. ~~Встроить аудио-энкодер. Ссылка: https://github.com/archinetai/archisound~~\n",
    "#### 5. ~~Реализовать сценарий обучения и обучить модель~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b2550-b41e-4952-b575-83bc645fc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.config import ModelConfig\n",
    "from models.unet import UNetWithCrossAttention\n",
    "from models.diffusion import Diffusion\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.SoundDataset import SoundDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877fb987-318f-43a5-a189-b77e18dead79",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/images\"\n",
    "sound_path = \"data/sounds\"\n",
    "\n",
    "data = SoundDataset(image_path, sound_path)\n",
    "train_data, val_data = torch.utils.data.random_split(data, [197889-10000, 10000])\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size=128,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True, \n",
    "                          drop_last=True)\n",
    "                         \n",
    "val_loader = DataLoader(val_data, \n",
    "                        batch_size=8,\n",
    "                        num_workers=8,\n",
    "                        pin_memory=True,\n",
    "                        shuffle=False, \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0d77ac-2261-4311-9381-6b465738e020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 11:02:06.942983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-19 11:02:06.950350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747641726.958814   35632 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747641726.961292   35632 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-19 11:02:06.970762: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from archisound import ArchiSound\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "autoencoder = ArchiSound.from_pretrained(\"dmae1d-ATC64-v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce172f84-80b8-4ab7-8046-80577e681ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig({\"image_size\": 128, \"audio_ctx_dim\": 32})\n",
    "\n",
    "# Инициализация\n",
    "diffusion = Diffusion(timesteps=1000, image_size=128, device=device)\n",
    "model = UNetWithCrossAttention(config).to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b17ff-7275-46a5-9e04-fafb7253dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e85b14516847d5828fd4c083358476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "unconditional_prob = 0.08\n",
    "\n",
    "epoch = 4\n",
    "for _ in tqdm(range(epoch)):\n",
    "    epo_train_losses = []\n",
    "    epo_val_losses = []\n",
    "    model.train()\n",
    "    for audio, images in train_loader:\n",
    "\n",
    "        if torch.rand(1) < unconditional_prob:\n",
    "            audio_embeds = None\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                audio_embeds = autoencoder.encode(audio.to(device)) # [B, d_audio, seq_len] [64, 32, 431]\n",
    "                \n",
    "            audio_embeds = audio_embeds.permute(0, 2, 1)  # [B, seq_len, d_audio]\n",
    "\n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epo_train_losses.append(loss.item())\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(sum(epo_train_losses)/len(epo_train_losses))\n",
    "    # валидация\n",
    "    model.eval()\n",
    "    for audio, images in val_loader:\n",
    "        with torch.no_grad():\n",
    "            audio_embeds = autoencoder.encode(audio.to(device))\n",
    "            \n",
    "            audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "\n",
    "            # audio_embeds = torch.zeros((8, 431, 32)).to(device)\n",
    "            images = images.to(device)\n",
    "            \n",
    "            loss = diffusion.loss_fn(model, images, audio_embeds)\n",
    "    \n",
    "            epo_val_losses.append(loss.item())\n",
    "\n",
    "    val_losses.append(sum(epo_val_losses)/len(epo_val_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a864a86-9131-407f-b4bb-176053a6f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edf337-680f-4177-ac93-82d915b06082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epo_train_losses, label='train')\n",
    "plt.plot(epo_train_losses, label='val')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6fe8c-7ea3-40cc-92b3-c783c89335ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio,image = next(iter(val_loader))\n",
    "audio_embeds = autoencoder.encode(audio.to(device))\n",
    "audio_embeds = audio_embeds.permute(0, 2, 1)\n",
    "\n",
    "model.eval()\n",
    "generated_image = diffusion.reverse_process(\n",
    "    model,\n",
    "    audio_embeds,\n",
    "    guidance_scale=7.5,\n",
    "    batch_size=8,\n",
    "    use_ddim=True,\n",
    "    timesteps=100  #число шагов\n",
    ")\n",
    "\n",
    "# 16 384\n",
    "# 32 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4586a27-8f05-4a99-9034-c82af78d8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.permute(generated_image[0].cpu()*0.5+0.5, (1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b3866-a522-4b9f-8557-d14a86b1541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# датасет обходится за 25 мин, 40 сек\n",
    "# при num_workers = 8 обходится за ~6 мин 30 сек\n",
    "# 1300 сек на эпоху\n",
    "\n",
    "# при двух attention блоках средние потери падают до 0.54 на 11 эпохах\n",
    "# при единственном блоке потери не пробивали отсечку в 0.9\n",
    "\n",
    "# 3 attention блока дали 0.84 на 4 эпохах. При продолжении обучения к концу 7 эпохи потери не изменились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55809c2-6374-47f9-ac97-f8b454a97cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for el in model.mid_block.attn.parameters():\n",
    "#     print(\"Attention stats:\", el.mean().item(), el.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df18674-2f31-4b76-9646-ac263bc6d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"weights2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd26e3-70fe-4336-9a42-511492c464e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e400c458-a24c-47a0-bf60-d2b23c6554a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e1f6fb-b669-4f0b-aef9-993f31de41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | down_blocks.0.conv1.weight: torch.Size([32, 3, 3, 3])\n",
      "1 | down_blocks.0.conv1.bias: torch.Size([32])\n",
      "2 | down_blocks.0.conv2.weight: torch.Size([32, 32, 3, 3])\n",
      "3 | down_blocks.0.conv2.bias: torch.Size([32])\n",
      "4 | down_blocks.0.BN1.weight: torch.Size([32])\n",
      "5 | down_blocks.0.BN1.bias: torch.Size([32])\n",
      "6 | down_blocks.0.downsample.weight: torch.Size([32, 32, 3, 3])\n",
      "7 | down_blocks.0.downsample.bias: torch.Size([32])\n",
      "8 | down_blocks.0.time_embed.0.weight: torch.Size([32, 1])\n",
      "9 | down_blocks.0.time_embed.0.bias: torch.Size([32])\n",
      "10 | down_blocks.0.time_embed.2.weight: torch.Size([32, 32])\n",
      "11 | down_blocks.0.time_embed.2.bias: torch.Size([32])\n",
      "12 | down_blocks.1.conv1.weight: torch.Size([64, 32, 3, 3])\n",
      "13 | down_blocks.1.conv1.bias: torch.Size([64])\n",
      "14 | down_blocks.1.conv2.weight: torch.Size([64, 64, 3, 3])\n",
      "15 | down_blocks.1.conv2.bias: torch.Size([64])\n",
      "16 | down_blocks.1.BN1.weight: torch.Size([64])\n",
      "17 | down_blocks.1.BN1.bias: torch.Size([64])\n",
      "18 | down_blocks.1.downsample.weight: torch.Size([64, 64, 3, 3])\n",
      "19 | down_blocks.1.downsample.bias: torch.Size([64])\n",
      "20 | down_blocks.1.time_embed.0.weight: torch.Size([64, 1])\n",
      "21 | down_blocks.1.time_embed.0.bias: torch.Size([64])\n",
      "22 | down_blocks.1.time_embed.2.weight: torch.Size([64, 64])\n",
      "23 | down_blocks.1.time_embed.2.bias: torch.Size([64])\n",
      "24 | down_blocks.2.conv1.weight: torch.Size([128, 64, 3, 3])\n",
      "25 | down_blocks.2.conv1.bias: torch.Size([128])\n",
      "26 | down_blocks.2.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "27 | down_blocks.2.conv2.bias: torch.Size([128])\n",
      "28 | down_blocks.2.BN1.weight: torch.Size([128])\n",
      "29 | down_blocks.2.BN1.bias: torch.Size([128])\n",
      "30 | down_blocks.2.downsample.weight: torch.Size([128, 128, 3, 3])\n",
      "31 | down_blocks.2.downsample.bias: torch.Size([128])\n",
      "32 | down_blocks.2.time_embed.0.weight: torch.Size([128, 1])\n",
      "33 | down_blocks.2.time_embed.0.bias: torch.Size([128])\n",
      "34 | down_blocks.2.time_embed.2.weight: torch.Size([128, 128])\n",
      "35 | down_blocks.2.time_embed.2.bias: torch.Size([128])\n",
      "36 | down_blocks.3.conv1.weight: torch.Size([256, 128, 3, 3])\n",
      "37 | down_blocks.3.conv1.bias: torch.Size([256])\n",
      "38 | down_blocks.3.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "39 | down_blocks.3.conv2.bias: torch.Size([256])\n",
      "40 | down_blocks.3.BN1.weight: torch.Size([256])\n",
      "41 | down_blocks.3.BN1.bias: torch.Size([256])\n",
      "42 | down_blocks.3.downsample.weight: torch.Size([256, 256, 3, 3])\n",
      "43 | down_blocks.3.downsample.bias: torch.Size([256])\n",
      "44 | down_blocks.3.time_embed.0.weight: torch.Size([256, 1])\n",
      "45 | down_blocks.3.time_embed.0.bias: torch.Size([256])\n",
      "46 | down_blocks.3.time_embed.2.weight: torch.Size([256, 256])\n",
      "47 | down_blocks.3.time_embed.2.bias: torch.Size([256])\n",
      "48 | mid_block_half.conv1.weight: torch.Size([128, 128, 3, 3])\n",
      "49 | mid_block_half.conv1.bias: torch.Size([128])\n",
      "50 | mid_block_half.BN1.weight: torch.Size([128])\n",
      "51 | mid_block_half.BN1.bias: torch.Size([128])\n",
      "52 | mid_block_half.attn.W_Q.weight: torch.Size([128, 128])\n",
      "53 | mid_block_half.attn.W_Q.bias: torch.Size([128])\n",
      "54 | mid_block_half.attn.W_K.weight: torch.Size([128, 32])\n",
      "55 | mid_block_half.attn.W_K.bias: torch.Size([128])\n",
      "56 | mid_block_half.attn.W_V.weight: torch.Size([128, 32])\n",
      "57 | mid_block_half.attn.W_V.bias: torch.Size([128])\n",
      "58 | mid_block_half.BN2.weight: torch.Size([128])\n",
      "59 | mid_block_half.BN2.bias: torch.Size([128])\n",
      "60 | mid_block_half.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "61 | mid_block_half.conv2.bias: torch.Size([128])\n",
      "62 | mid_block_bot.conv1.weight: torch.Size([256, 256, 3, 3])\n",
      "63 | mid_block_bot.conv1.bias: torch.Size([256])\n",
      "64 | mid_block_bot.BN1.weight: torch.Size([256])\n",
      "65 | mid_block_bot.BN1.bias: torch.Size([256])\n",
      "66 | mid_block_bot.attn.W_Q.weight: torch.Size([256, 256])\n",
      "67 | mid_block_bot.attn.W_Q.bias: torch.Size([256])\n",
      "68 | mid_block_bot.attn.W_K.weight: torch.Size([256, 32])\n",
      "69 | mid_block_bot.attn.W_K.bias: torch.Size([256])\n",
      "70 | mid_block_bot.attn.W_V.weight: torch.Size([256, 32])\n",
      "71 | mid_block_bot.attn.W_V.bias: torch.Size([256])\n",
      "72 | mid_block_bot.BN2.weight: torch.Size([256])\n",
      "73 | mid_block_bot.BN2.bias: torch.Size([256])\n",
      "74 | mid_block_bot.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "75 | mid_block_bot.conv2.bias: torch.Size([256])\n",
      "76 | up_blocks.0.upsample.weight: torch.Size([512, 512, 3, 3])\n",
      "77 | up_blocks.0.upsample.bias: torch.Size([512])\n",
      "78 | up_blocks.0.conv1.weight: torch.Size([128, 512, 3, 3])\n",
      "79 | up_blocks.0.conv1.bias: torch.Size([128])\n",
      "80 | up_blocks.0.BN1.weight: torch.Size([128])\n",
      "81 | up_blocks.0.BN1.bias: torch.Size([128])\n",
      "82 | up_blocks.0.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "83 | up_blocks.0.conv2.bias: torch.Size([128])\n",
      "84 | up_blocks.0.time_embed.0.weight: torch.Size([128, 1])\n",
      "85 | up_blocks.0.time_embed.0.bias: torch.Size([128])\n",
      "86 | up_blocks.0.time_embed.2.weight: torch.Size([128, 128])\n",
      "87 | up_blocks.0.time_embed.2.bias: torch.Size([128])\n",
      "88 | up_blocks.1.upsample.weight: torch.Size([256, 256, 3, 3])\n",
      "89 | up_blocks.1.upsample.bias: torch.Size([256])\n",
      "90 | up_blocks.1.conv1.weight: torch.Size([64, 256, 3, 3])\n",
      "91 | up_blocks.1.conv1.bias: torch.Size([64])\n",
      "92 | up_blocks.1.BN1.weight: torch.Size([64])\n",
      "93 | up_blocks.1.BN1.bias: torch.Size([64])\n",
      "94 | up_blocks.1.conv2.weight: torch.Size([64, 64, 3, 3])\n",
      "95 | up_blocks.1.conv2.bias: torch.Size([64])\n",
      "96 | up_blocks.1.time_embed.0.weight: torch.Size([64, 1])\n",
      "97 | up_blocks.1.time_embed.0.bias: torch.Size([64])\n",
      "98 | up_blocks.1.time_embed.2.weight: torch.Size([64, 64])\n",
      "99 | up_blocks.1.time_embed.2.bias: torch.Size([64])\n",
      "100 | up_blocks.2.upsample.weight: torch.Size([128, 128, 3, 3])\n",
      "101 | up_blocks.2.upsample.bias: torch.Size([128])\n",
      "102 | up_blocks.2.conv1.weight: torch.Size([32, 128, 3, 3])\n",
      "103 | up_blocks.2.conv1.bias: torch.Size([32])\n",
      "104 | up_blocks.2.BN1.weight: torch.Size([32])\n",
      "105 | up_blocks.2.BN1.bias: torch.Size([32])\n",
      "106 | up_blocks.2.conv2.weight: torch.Size([32, 32, 3, 3])\n",
      "107 | up_blocks.2.conv2.bias: torch.Size([32])\n",
      "108 | up_blocks.2.time_embed.0.weight: torch.Size([32, 1])\n",
      "109 | up_blocks.2.time_embed.0.bias: torch.Size([32])\n",
      "110 | up_blocks.2.time_embed.2.weight: torch.Size([32, 32])\n",
      "111 | up_blocks.2.time_embed.2.bias: torch.Size([32])\n",
      "112 | up_blocks.3.upsample.weight: torch.Size([64, 64, 3, 3])\n",
      "113 | up_blocks.3.upsample.bias: torch.Size([64])\n",
      "114 | up_blocks.3.conv1.weight: torch.Size([3, 64, 3, 3])\n",
      "115 | up_blocks.3.conv1.bias: torch.Size([3])\n",
      "116 | up_blocks.3.BN1.weight: torch.Size([3])\n",
      "117 | up_blocks.3.BN1.bias: torch.Size([3])\n",
      "118 | up_blocks.3.conv2.weight: torch.Size([3, 3, 3, 3])\n",
      "119 | up_blocks.3.conv2.bias: torch.Size([3])\n",
      "120 | up_blocks.3.time_embed.0.weight: torch.Size([3, 1])\n",
      "121 | up_blocks.3.time_embed.0.bias: torch.Size([3])\n",
      "122 | up_blocks.3.time_embed.2.weight: torch.Size([3, 3])\n",
      "123 | up_blocks.3.time_embed.2.bias: torch.Size([3])\n",
      "124 | norm.weight: torch.Size([3])\n",
      "125 | norm.bias: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    print(f\"{i} | {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ca3cb-58f4-46f1-8632-336a88c35731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
